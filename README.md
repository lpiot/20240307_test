This project uses a `GitPod` environment.  

Click the button below to start a new development environment based upon **test** `git` branch:

[![Open in Gitpod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io/#https://github.com/lpiot/gitpod-workspace/tree/test)

# Notes

## Virtualisation

### Objectifs et difficult√©s

üéØ L'objectif **principal** de la virtualisation est de consolider (_i.e._ optimiser) l'utilisation des serveurs et donc diminuer le rapport co√ªt/usage.  
Cette d√©marche a diff√©rents effets ind√©sirables qu'il va falloir g√©rer, adresser¬†:

- comment faire cohabiter sur un m√™me serveur des _stacks_ techniques incompatibles entre elles (ex. faire cohabiter `PHP 7.3` et `PHP 8.2`)¬†?
   - en jouant sur le `PATH` expos√© dans la session qui lance les processus applicatifs, on peut faire cohabiter plusieurs _stacks_ (c'est le principe des outils comme `virtualenv` en `Python`). Mais c'est tr√®s p√©nible √† g√©rer car il faut beaucoup d'expertise et de tests pour arriver √† le configurer.
- comment isoler les _workloads_ applicatives d'un client/d'un _tenant_ par rapport aux autres¬†?
   - au niveau _filesystem_ (les droits `UGO` sont une premi√®re r√©ponse. `chroot` est un niveau de r√©ponse renforc√©)
     ![chroot schema](/images/chroot.jpg)
   - au niveau r√©seau
   - au niveau processus
   - au niveau du quota de consommation de ressources par chaque processus
- comment g√©rer sans d√©ployer une √©norme expertise sysadmin¬†?

### Fonctionnement sous-jacent

La virtualisation _per se_ se base sur une **redirection** des appels syst√®me adress√©s √† du mat√©riel _virtuel_ vers le mat√©riel physique r√©el.  
Ainsi une VM poss√®de¬†:

- son propre CPU (avec tout ou partie des capacit√©s du CPU mat√©riel de l'hyperviseur)
- sa propre carte r√©seau
- son propre disque
- etc.

#### Inconv√©nients

Cette redirection co√ªte du temps

- pour router les messages vers le bon mat√©riel,
- pour traduire les messages destin√©s √† un type de mat√©riel virtuel en messages destin√©s au mat√©riel physique (ex. carte r√©seau Intel E1000 vers carte r√©seau physique)
- pour disposer de temps d'ex√©cution sur le mat√©riel en question

La virtualisation est aussi co√ªteuse en effort d'administration puisque chaque VM r√©clame l'effort d'administration d'un serveur.

#### Avantages

Malgr√© tout, la virtualisation apporte une certaine souplesse de manipulation.  
On peut

- cloner une _VM_,
- la d√©placer d'un _hyperviseur_ √† un autre
- r√©duire la place disque, voire la place en m√©moire RAM utilis√©es par des _VM_s clon√©es (fonctionnalit√© de _copy-on-write_)
- piloter ces manipulations sous forme de scripts et d'automatisations

## Containerisation

La _containerisation_ est un cousin √©loign√© de la virtualisation.  
En se basant sch√©matiquement sur les m√™mes principes de redirection d'appels syst√®me,

- mais plus au niveau de mat√©riel virtuel pointant vers du mat√©riel physique
- mais au sein m√™me de l'_OS_, avec des m√©canismes d'isolation que sont les `namespaces` et les `cgroups`.

En utilisant ces isolations √† une √©chelle plus fine, sur des √©l√©ments plus petits que des composants physique, on dispose d'une technologie de gestion des processus isol√©s beaucoup plus l√©g√®re, v√©loce et peu consommatrice en ressources mat√©rielles et en effort d'administration.

En plus de cela, la technologie des _container images_ permet de **standardiser** la mani√®re dont on manipule une _workload_ applicative¬†:

- dans la mani√®re de la _package_ et de la distribuer
- dans la mani√®re de la _deploy_
- dans la mani√®re de g√©rer son cycle de vie (d√©marrage / arr√™t / pause / suppression)
- dans la mani√®re dont on _monitor_ son activit√©

![software stack](/images/software-stack.jpg)

# Glossaire

## blue/green deployment - canary testing/deployment - rolling update

Lorsque la _workload_ est distribu√©e sur une architecture technique √† scalabilit√© horizontale, sa r√©partition permet d'envisager des **strat√©gies de d√©ploiement sphistiqu√©es**, qui permettent de limiter les effets n√©gatifs en cas d'incident au d√©ploiement / √† l'_upgrade_. Voire de masquer compl√®tement l'op√©ration aux utilisateurs.

Le _blue/green d√©ployment_ consiste √† d√©ployer la nouvelle version (_green_) sur un sous-ensemble des n≈ìuds qui servent la _workload_ applicative (initialement en version _blue_). Ainsi, si 10% des n≈ìuds sont en version _green_, seuls 10% des utilisateurs sont touch√©s par cette mise √† jour. On peut alors contr√¥ler que tout va bien pour cette sous-population de "beta-testeurs" qui s'ignorent et refaire la m√™me op√©ration sur 10% de n≈ìuds compl√©mentaires‚Ä¶ jusqu'√† avoir remplac√© 100% des n≈ìuds servant _blue_ par des n≈ìuds servant _green_.  
On parle aussi de _rolling-update_, dans le sens que cette mise √† jour se fait de mani√®re it√©rative sur l'ensemble des n≈ìuds, en les traitant par √©chantillons, par sous-ensembles.

Le _canary testing_ ou _canary deployment_ est une variante de cette pratique. La m√©thodologie est la m√™me. Mais l'objectif est diff√©rent.  
Ici, on souhaite exposer une version √† une population restreinte d'utilisateurs, √† des fins de tests, d'analyse de r√©action du march√©, etc. Dans ce cas, on n'adresse pas l'ensemble des n≈ìuds, mais on s'en tient √† un petit sous-ensemble qui servira d'√©chantillon pour tests. Des beta-testeurs √† pas cher¬†!

![canary testing schema](/images/canary-testing.jpg)

## copy-on-write

Tr√®s utilis√©e dans le monde de la virtualisation, du _Cloud_ et des _containers_.  
Il s'agit d'une fonctionnalit√© avanc√©e des _filesystems_, qui permet de conserver diff√©rentes versions d'un m√™me fichier, non pas en copiant int√©gralement le fichier v1 dans sa version v2, mais en ne recopiant que la portion qui conna√Æt une √©volution / une alt√©ration.  
C'est aussi le principe qui est utilis√© dans les fonctions de d√©duplication des baies de stockage.

Pour sch√©matiser, on aura donc¬†:

- le fichier ludo-rox-v1.txt en v1
- un pointeur de fichier ludo-rox-v2.txt qui pointera vers la v1, sauf pour la ligne 42, o√π le texte a chang√©.

Ce fonctionnement est valable pour tout type de fichier, avec un impact quasi inexistant sur les performances.  
Gros avantage¬†: si on a 30¬†versions du m√™me fichier avec, √† chaque fois, 1¬†seule ligne qui varie,

- l'espace de stockage consomm√© ne sera pas 30¬†fois la taille du fichier,
- mais 1¬†fois la taille du fichier + 30¬†lignes alt√©r√©es.

Gros gain de place √† la cl√©.  
Gros gain en latence lorsqu'on doit repartir d'un fichier initial qu'on aurait d√ª dupliquer (dans le cas de clonage de disque de VMs, c'est un gain substentiel qui peut se compter en minutes gagn√©es¬†!)

## hyperviseur

C'est le logiciel qui permet de g√©rer la virtualisation sur un serveur.  
Par extension, on parle aussi d'hyperviseur pour d√©signer un serveur sur lequel on d√©ploie de la virtualisation et des VMs.

## scale-out / scalabilit√© horizontale

Quand on veut augmenter la puissance disponible pour servir une _workload_, on a plusieurs strat√©gies.  
On peut augmenter le nombre de serveurs qui peuvent servir la _workload_. Dans ce cas, on parle de scalabilit√© horizontale ou _scale out_.  
Les requ√™tes soumises par les utilisateurs sont r√©parties sur l'ensemble des n≈ìuds (i.e. des serveurs), qui servent la _workload_ applicative. Ainsi, chaque n≈ìud ne sert qu'une partie des utilisateurs.

![scale out schema](/images/scale-out.jpg)

## scale-up / scalabilit√© verticale

Quand on veut augmenter la puissance disponible pour servir une _workload_, on a plusieurs strat√©gies.  
On peut augmenter la puissance du serveur, qui sert la _workload_, en ajoutant de la RAM, du disque, des processeurs, en rempla√ßant les disques par d'autres plus v√©loces (SSD / flash media). Dans ce cas, on parle de scalabilit√© verticale ou _scale up_.  
Ce n'est pas toujours faisable¬†:

- le serveur doit rester _up_ et l'ajout de ressources n√©cessite un red√©marrage
- le serveur est d√©j√† dot√© au maximum, plus aucun slot m√©moire ou socket CPU libre

Et surtout, si √ßa r√©pond (au moins temporairement) aux besoins accrus de performance, √ßa ne r√©pond pas aux besoins de r√©silience¬†: si le serveur conna√Æt un incident, la _workload_ ne peut pas √™tre r√©partie sur un autre serveur.

## tenant

C'est une instance logique (dont la nature fonctionnelle est variable) que l'on souhaite isoler des autres instances.  
On parle de tenants pour distinguer

- les applications d√©di√©es √† un client ou un autre,
- les diff√©rents environnements techniques d'une m√™me application
- les diff√©rents "espaces de travail" au sein d'une m√™me application

## workload (applicative)

Litt√©ralement, la charge de travail. En _IT_, on parle de _workload_ pour √©voquer les programmes applicatifs qui sont **en cours d'ex√©cution** sur le serveur.


# Color. go

## Contexte

Il s'agit d'une application √©crite en `Golang`.  
Cette application lance un serveur _Web_ en `HTTP` (port 80 par d√©faut, configurable dans la variable d'environnement `PORT`).

## üéØ Objectifs

1. Comprendre ce que fait l'application
1. Compiler l'application
   - en local sur la machine `Gitpod`
  
    ```bash
    cd /workspace/20240307_test/color
    go build -o ./color ./color.go
    ```

   - dans un _container_ en mode interactif
   - dans un _container_ bas√© sur une image _custom_ sp√©cialement cr√©√©e pour le besoin
   - dans un _container_ g√©n√©rique d√©di√© √† la compilation de binaires `Golang`.
    ```bash
    docker container run --volume /workspace/20240307_test/color:/app golang go build -o /app/color /app/color.go
    ```
 
2. R√©cup√©rer le binaire ex√©cutable
3. Lancer le binaire
   - en local sur la machine `Gitpod`
  
    ```bash
    cd /workspace/20240307_test/color
    go run ./color.go
    ```

    ou

    ```bash
    /workspace/20240307_test/color/color
    ```
   - dans un _container_ bas√© sur une image _custom_ sp√©cialement cr√©√©e pour le besoin
    ```bash
    echo << EOF
    FROM busybox

    WORKDIR /
    COPY ./color /

    EXPOSE 80
    CMD /color
    EOF

    docker build --tag my-go-image .
    docker run -P my-go-image
    ```
